# MODEL
BART는 손상된 문서를 원본 문서에 매핑하는 노이즈 제거 자동 인코더이다.
손상된 텍스트에 대한 양방향 인코더와 왼쪽에서 오른쪽으로 자동 회귀 디코더가 있는 시퀀스-시퀀스 모델로 구현된다.
사전 교육을 위해 원본 문서의 음수 로그 가능성을 최적화한다.
# 아키텍쳐
BART가에서 표준 시퀀스에 시퀀스 변압기 아키텍처를 사용 vaswani : 2017
GeLUs에 ReLU 활성화 기능을 수정하는 것이, GPT에 따라 제외 ( 겔루 ) 에서와 초기화 매개 변수 N ( 0 , 0.02 ). 기본 모델의 경우 인코더와 디코더에 6 개의 레이어를 사용하고 큰 모델의 경우 각각 12 개의 레이어를 사용
 아키텍처는 BERT에서 사용되는 것과 밀접한 관련이 있으며 다음과 같은 차이점이 있음
BERT는 단어 예측 전에 추가 피드 포워드 네트워크를 사용하지만 BART는 그렇지 않음
RT에는 동일한 크기의 BERT 모델보다 대략 10 % 더 많은 매개 변수가 포함
# 사전 훈련
BART는 문서를 손상시킨 다음 디코더의 출력과 원본 문서 사이의 교차 엔트로피 인 재구성 손실을 최적화홤으로써 학습된다. 
특정 소음 체계에 맞게 조정 된 기존 소음 제거 자동 인코더와 달리 BART를 사용하면 모든 유횽의 문서 손상을 적용 할 수 있음
소스에 대한 모든 정보가 손실되는 극단적인 경우 BART는 언어 모델과 같다.
BERT bert 다음 에 임의 토큰이 샘플링되고 [MASK] 요소 로 대체된다.
입력에서 임의 토큰이 삭제, 토큰 마스킹과 달리 모델은 입력이없는 위치를 결정 해야함
Poisson 분포 ( λ = 3 ) 에서 스팬 길이를 사용하여 여러 텍스트 스팬이 샘플링 된다.
각 범위는 단일 [MASK] 토큰으로 대체됩니다 . 길이가 0 인 범위는 [MASK] 토큰 삽입에 해당된다.
텍스트 채움은 SpanBERT spanbert 에서 영감을 얻었 지만 SpanBERT 샘플은 길이가 다른 (클램프 된 기하학적) 분포에서 길이를 확장하며 각 범위 를 정확히 동일한 길이 의 [MASK] 토큰 시퀀스로 대체한다.
텍스트 채움은 모델이 범위에서 누락 된 토큰 수를 예측하도록 지시한다.
문서는 전체 정지를 기준으로 문장으로 구분되며이 문장은 임의 순서로 섞임
토큰은 무작위로 균일하게 선택되며 문서는 해당 토큰으로 시작하도록 회전됩니다. 이 작업은 모델이 문서의 시작을 식별하도록 훈련시킵니다.
# 미세 조정 BART
BART에 의해 생성 된 표현은 다운 스트림 애플리케이션에 여러 가지 방식으로 사용될 수 있다.
# 서열 분류 작업
시퀀스 분류 작업의 경우 동일한 입력이 인코더와 디코더에 공급되고 최종 디코더 토큰의 최종 숨겨진 상태는 새로운 멀티 클래스 선형 분류기에 공급된다.
이 접근법은 BERT의 CLS토큰과 관련이 있다.
디코더의 토큰 표현이 완전한 입력에서 디코더 상태에 참여할 수 있도록 끝에 토큰을 추가
# 토큰 분류 작업
SQuAD에 대한 응답 종점 분류와 같은 토큰 분류 작업의 경우 완전한 문서를 인코더와 디코더에 공급하고
디코더의 숨겨진 숨겨진 상태를 각 단어의 표현으로 사용한다
토큰을 분류하는 데 사용된다
#시퀀스 생성 작업
BART에는 자동 회귀 디코더가 있기 때문에 추상적 질문 응답 및 요약과 같은 시퀀스 생성 작업에 맞게 직접 조정할 수 있음
이 두 가지 작업에서 정보는 입력에서 복사되지만 조작되지만 이는 노이즈 제거 사전 훈련 목표와 밀접한 관련이 있음
인코더 입력은 입력 시퀀스이고, 디코더는 자동 회귀 적으로 출력을 생성
# 기계 번역
bitext에서 배운 새로운 인코더 매개 변수 세트를 추가하여 기계 번역을위한 단일 사전 훈련 된 디코더로 전체 BART 모델 (인코더 및 디코더 모두)을 사용할 수 있음을 보여준다
BART의 엔코더 임베딩 레이어를 무작위로 초기화 된 새로운 엔코더로 대체
이 모델은 엔드 투 엔드로 훈련되며, 새로운 인코더가 외국어를 BART가 영어로 노이즈 제거 할 수있는 입력에 매핑하도록 훈련시킴
새로운 엔코더는 원래 BART 모델과 다른 어휘를 사용할 수 있습니다.
